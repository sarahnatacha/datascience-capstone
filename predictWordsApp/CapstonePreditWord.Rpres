<style>

body {
  background: white;
  background-color: white; 
}

.footer {
    color: black;
    background: #E8E8E8;
    position: fixed;
    top: 90%;
    text-align:center;
    width:100%;
}

.reveal h1, .reveal h2, .reveal h3 {
  word-wrap: normal;
  moz-hyphens: none;
  color: black;
}

</style>


Data Science Capstone: Prediction of the next word
========================================================
author: Sarah Natacha
date: March 20th 2017
autosize: true

Project Overview
========================================================
This project is based on files named LOCALE.blogs.txt where LOCALE is the each of the four locales en_US, de_DE, ru_RU and fi_FI. The data is from a corpus called HC Corpora (www.corpora.heliohost.org). 
The data is downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
The goal of this project is to build a Shiny app that takes as input a phrase (multiple words), and upon submit predicts the next word(s).

The algorithm
========================================================
The algorithm predicts the next US word based on the highest probability. The data contains three corpora of news, tweets and blogs with more than 3M lines. 
- We took a 5% sample of the three files 
- The sample was cleaned by removing the special characters, emails, white spaces,spaces, bad words
- The sample was tokenized to form tables of sequences of 5 n-grams and sorted by frequency. The ngrams were used to predict the next words based on the user input.

The code can be found at https://github.com/sarahnatacha/datascience-capstone/tree/master/predictWordsApp

Application
========================================================
The application is a Shiny app that:
- Predicts the next US word based on the highest probability
- The word or sentence is entered in a text box

The application can be live tested at https://sarahnatacha.shinyapps.io/predictWordsApp/

![Shiny app](shinyapp.jpg)

Performance and comments
========================================================
The accuracy of the result depends on the sample that was used for the tokenization. 
It was a challenge to create the ngrams and we had to reduce the size of the sample since it was giving out of memory errors or was taking too long to create. Therefore, we used n-grams that was small but if we were using a larger sample it would provide more options for the next word results. 


